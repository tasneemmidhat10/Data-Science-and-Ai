{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66a59e6-34a1-443c-8d53-d3e898f3cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c434a546-a06e-4839-875f-22b1790c782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"reviews = pd.read_csv('reviews_with_splits.csv')\\nreviews\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"reviews = pd.read_csv('reviews_with_splits.csv')\n",
    "reviews\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec5b5f-8ed4-4f24-acb6-9c006d0ed237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669f8d60-9608-489c-a760-6dd91b59e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"by_rating = collections.defaultdict(list)\n",
    "''' Iterating over the rows of the data frame using the iterrows method, to group the data by the rating then spliiting it inot train-test-val'''\n",
    "for _, row in reviews.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb64b2-1dae-48ea-a636-b30ce0f763e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9fedc8-96eb-4117-aed2-c5022b2954a9",
   "metadata": {},
   "source": [
    "The data set is 56000 training example, with a rating, review and split features. We need to group the rows by the rating using defaultdict method from the collections class. It outputs a list of dictinaries grouped by the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e050dc53-d8e6-42aa-810a-37b38644cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_function(data, train_proportion, val_proportion, test_proportion, seed):\n",
    "    final_list =[]\n",
    "    np.random.seed(seed)\n",
    "    for key, item_list in data.items():\n",
    "        np.random.shuffle(item_list)\n",
    "        n_total = len(data[key])\n",
    "        n_train = int(train_proportion*n_total)\n",
    "        n_val = int(val_proportion*n_total)\n",
    "        n_test = int(test_proportion*n_total)\n",
    "        for item in item_list[:n_train]:\n",
    "            item['split'] = 'train'\n",
    "        for item in item_list[n_train:n_train+n_val]:\n",
    "            item['split'] = 'val'\n",
    "        for item in item_list[n_val+n_train:n_val+n_test+n_test]:\n",
    "             item['split'] = 'test'\n",
    "        final_list.extend(item_list)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c75c046-800c-436e-bae3-fc52b2021c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"final_reviews = data_split_function(by_rating, 0.75, 0.15, 0.15, 42)\n",
    "\"\"\"\n",
    "final_reviews = pd.read_csv('final_reviews_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54d3d4b-9be5-45bc-b229-560169e1164c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>i signed online to schedule an appt . days pri...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>mct oil buy get the second half off . good dea...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>food here is mediocre and tastes super artific...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>server was not efficient and didn t seem to ha...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>service wasn t that great . the prime rib has ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>positive</td>\n",
       "      <td>done a service repair for a broken sub . . fas...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>positive</td>\n",
       "      <td>not bad sushi . my husband and i went there ab...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>positive</td>\n",
       "      <td>we came here last night for dinner . we though...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>positive</td>\n",
       "      <td>all i have to say is start with the pineapple ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>positive</td>\n",
       "      <td>taco bell must offer some of the cheapest calo...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                             review  split\n",
       "0      negative  i signed online to schedule an appt . days pri...  train\n",
       "1      negative  mct oil buy get the second half off . good dea...  train\n",
       "2      negative  food here is mediocre and tastes super artific...  train\n",
       "3      negative  server was not efficient and didn t seem to ha...  train\n",
       "4      negative  service wasn t that great . the prime rib has ...  train\n",
       "...         ...                                                ...    ...\n",
       "55995  positive  done a service repair for a broken sub . . fas...   test\n",
       "55996  positive  not bad sushi . my husband and i went there ab...  train\n",
       "55997  positive  we came here last night for dinner . we though...  train\n",
       "55998  positive  all i have to say is start with the pineapple ...  train\n",
       "55999  positive  taco bell must offer some of the cheapest calo...  train\n",
       "\n",
       "[56000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #This is a list of dictionaries and will be transformed to a pandas dataframe\n",
    "#final_reviews = pd.DataFrame(final_reviews)\n",
    "final_reviews = final_reviews.drop('Unnamed: 0', axis='columns')\n",
    "final_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1e4c51-9e81-4886-9aaf-d85c41520a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "\"\"\"def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([. , ! ?])\", r\"\\1\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z., !?]+\", r\" \", text)\n",
    "    return text\n",
    "final_reviews.review = final_reviews.review.apply(preprocess_text)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975814ab-f20c-4c13-b1e7-bf6c9f985e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_reviews.to_csv('final_reviews_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "437b03e1-51f5-4cbf-a19a-f261fbfb06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35025df8-17f2-4608-b5ad-97cd28d21131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token='<UNK>'):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx:token for token, idx in token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    def lookup_token(self, token):\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"The index (%d) is not in the Vocabulary\" %index)\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size = %d>\" % len(self)\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896ed484-3f43-49b7-b22f-912069e7e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "    def vectorize(self, review):\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype = np.float32)\n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff = 25):\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk= False)\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word]+=1\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        return cls(review_vocab = review_vocab, rating_vocab = rating_vocab)\n",
    "    def to_serializable(self):\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f09e3c1-0dcb-4cf0-93c8-11bde271a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e602c5a-837e-4bec-8c27-ddb00ad2702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = num_features, out_features = 1)\n",
    "    def forward(self, x_in, apply_sigmoid = False):\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3133d531-bf72-4e40-ac8b-e379d880f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ReviewMLP(nn.Module):\n",
    "    def __inti__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\" This is a multiLayer perceptron with two hidden layers \n",
    "        The input is a tensor with input_dim, the output of the first layer will be stored \n",
    "        in a hidden vector of the size hidden_dim * 1, and the output score vector will be of the size output_dim * 1 \"\"\"\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(input_dim, hidden_dim)\n",
    "    def forward(self, x_in, apply_softmax = False):\n",
    "        layer1_output = self.fc1(x_in)\n",
    "        hidden_vector = F.relu(layer1_output)\n",
    "        prediction_vector = self.fc2(hidden_vector)\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "        return prediction_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7403ab4d-8865-483e-b383-654aec99a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace( model_state_file = 'model.pth', review_csv = 'final_reviews_csv', sav_dir = 'Home', vetorizer_file = 'vectorizer.json',\n",
    "                 hidden_dim = 300,\n",
    "                 batch_size = 128,\n",
    "                 early_stopping_criteria = 5,\n",
    "                 learning_rate = 0.001,\n",
    "                 num_epochs = 100,\n",
    "                 seed = 1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dec6fa9f-951d-4650-ac7c-88ff0920691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import string\n",
    "def make_train_state(args):\n",
    "    return {'epoch_index':0, \n",
    "            'train_loss':[],\n",
    "            'train_acc':[],\n",
    "            'val_loss':[],\n",
    "            'val_acc':[],\n",
    "            'test_loss':-1,\n",
    "            'test_acc':-1}\n",
    "train_state = make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = ReviewClassifier(num_features = len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)\n",
    "def compute_accuracy(y_predicted, y_target):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total+= y_target.size(0)\n",
    "    correct+= (y_predicted == y_target).sum().item()\n",
    "    accuracy = (correct/total) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86730db6-686a-4ac3-a6af-ff3a852e9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, args.batch_size)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'].float())\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index+1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index+1)\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size = args.batch_size)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'].float())\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index+1)\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc+= (acc_batch - running_acc) / (batch_index + 1)\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b158469-7275-4725-ba2f-f0a79cda23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred = classifier(x_in = batch_dict['x_data'].float())\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    batch_loss = loss.item()\n",
    "    running_loss += (batch_loss- running_loss) / (batch_index+1)\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (batch_loss - running_acc) / (batch_index+1)\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77e57fc7-75e1-4af0-b908-32380120ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.324\n",
      "Test accuracy: 0.32\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {:.3f}'.format(train_state['test_loss']))\n",
    "print('Test accuracy: {:.2f}'.format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5bfb3fa-c078-41f0-ab83-e6b310482f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really had a good experience with this restaurant, but there are some negative sides to it, first: The atmosphere was hot and noisy, second : I did not  like the toast they offer, overall a decent resturant -> negative\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold = 0.5):\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "    return vectorizer.rating_vocab.lookup_index(index)\n",
    "test_review = 'I really had a good experience with this restaurant, but there are some negative sides to it, first: The atmosphere was hot and noisy, second : I did not  like the toast they offer, overall a decent resturant'\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "print('{} -> {}'.format(test_review, prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fa5381c-8a89-40aa-aa1b-50f7f2bae4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infuential words in positive reviews: \n",
      "chinatown\n",
      "pleasantly\n",
      "mmmmmm\n",
      "deliciousness\n",
      "nclean\n",
      "eclectic\n",
      "hooked\n",
      "amazed\n",
      "artsy\n",
      "spotless\n",
      "heavenly\n",
      "nexcellent\n",
      "nhighly\n",
      "stunning\n",
      "keeper\n",
      "chapel\n",
      "mmmm\n",
      "awesomeness\n",
      "coma\n",
      "nthank\n"
     ]
    }
   ],
   "source": [
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending = True)\n",
    "indices = indices.numpy().tolist()\n",
    "print('Infuential words in positive reviews: ')\n",
    "\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84e5d5bb-89e3-4928-8ee0-e15bce13552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influntial words in negative review\n",
      "slowest\n",
      "cancelled\n",
      "unacceptable\n",
      "nmaybe\n",
      "underwhelmed\n",
      "operator\n",
      "meh\n",
      "worst\n",
      "subject\n",
      "canceled\n",
      "blech\n",
      "insulting\n",
      "gossiping\n",
      "mediocre\n",
      "burden\n",
      "horrendous\n",
      "awful\n",
      "embarrassing\n",
      "receipts\n",
      "bland\n"
     ]
    }
   ],
   "source": [
    "print('Influntial words in negative review')\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d29bf-a73d-4e0d-aab6-ead9914e1c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
